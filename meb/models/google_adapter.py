import numpy as np
import hashlib
import os
import asyncio
import onnxruntime as ort
import logging
import time
from typing import List, Optional, Tuple, Union, Dict, Any
from .encoders import BaseEncoder, get_encoder

class GoogleAdapter(BaseEncoder):
    """
    Encoder that applies ONNX-based transformations to base embeddings.
    Uses a trained ONNX model for embedding transformations.
    """
    
    def __init__(self, base_encoder: str = "mock", model_path: str = None, name: str = "google_adapter", cache_dir: str = "cache"):
        """
        Initialize ONNX-based adapter encoder.
        
        Args:
            base_encoder: Base encoder to use for initial embeddings
            model_path: Path to your ONNX model (if None, uses default path)
            name: Name for caching
            cache_dir: Directory to store cached embeddings
        """
        super().__init__(name, cache_dir)
        self.base_encoder_name = base_encoder
        self.model_path = model_path or "utils/models/adapter.onnx"
        self.session = self._load_model_adapter()
        
        # Initialize base encoder
        self.base_encoder = get_encoder(base_encoder)
                
    def _load_model_adapter(self):
        """
        Load the ONNX model for inferencing embeddings.
        
        Returns:
        - session (onnxruntime.InferenceSession): The loaded ONNX model session.
        """
        # Use the configured model path
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ONNX model not found at: {self.model_path}")
        
        # Load the ONNX model
        session = ort.InferenceSession(self.model_path)
        logging.info(f"Loaded ONNX model from: {self.model_path}")
        return session
    
    async def _generate_new_embedding(self, embed_gcp):
        """
        Generate new embeddings using the ONNX model.

        Parameters:
        - session (onnxruntime.InferenceSession): The loaded ONNX model session.
        - embed_gcp (list or np.array): The input embedding to be processed.

        Returns:
        - new_embeddings_normalized (np.array): The normalized new embeddings generated by the model.
        """
        start_time = time.time()
        logging.info(f"Starting to generate new embeddings")

        input_name = self.session.get_inputs()[0].name  # Get the input name for the ONNX model
        output_name = self.session.get_outputs()[0].name  # Get the output name for the ONNX model

        array_embed_gcp = np.array(embed_gcp, ndmin=2)  # Ensure the input embedding is in a 2D array format

        # Run the model to generate new embeddings
        new_embeddings = self.session.run([output_name], {input_name: array_embed_gcp})[0][0]

        # Normalize the generated embeddings
        new_embeddings_normalized = new_embeddings / np.linalg.norm(new_embeddings)

        end_time = time.time()
        logging.info(f"New embeddings generated in {end_time - start_time:.4f} seconds")

        return new_embeddings_normalized
    
    def _generate_new_embedding_sync(self, embed_gcp):
        """
        Synchronous version of _generate_new_embedding for use in the main pipeline.
        
        Parameters:
        - embed_gcp (np.array): The input embedding to be processed.
        
        Returns:
        - new_embeddings_normalized (np.array): The normalized new embeddings.
        """
        input_name = self.session.get_inputs()[0].name
        output_name = self.session.get_outputs()[0].name
        
        array_embed_gcp = np.array(embed_gcp, ndmin=2)
        
        # Run the model to generate new embeddings
        new_embeddings = self.session.run([output_name], {input_name: array_embed_gcp})[0][0]
        
        # Normalize the generated embeddings
        new_embeddings_normalized = new_embeddings / np.linalg.norm(new_embeddings)
        
        return new_embeddings_normalized
    
    def _apply_adapter(self, embeddings: np.ndarray) -> np.ndarray:
        """
        Apply ONNX model transformation to embeddings.
        
        Args:
            embeddings: Input embeddings (n_items, embedding_dim)
            
        Returns:
            Transformed embeddings
        """
        if embeddings.size == 0:
            return embeddings
        
        # Apply transformation to each embedding
        transformed_embeddings = []
        for embedding in embeddings:
            transformed = self._generate_new_embedding_sync(embedding)
            transformed_embeddings.append(transformed)
        
        return np.array(transformed_embeddings)
    
    def encode_text(self, texts: List[str]) -> np.ndarray:
        """
        Encode text inputs using base encoder + ONNX transformation.
        
        Args:
            texts: List of text strings
            
        Returns:
            Array of transformed text embeddings
        """
        # Get base embeddings
        base_embeddings = self.base_encoder.encode_text(texts)
        
        # Apply ONNX transformation
        return self._apply_adapter(base_embeddings)
    
    def encode_image(self, image_paths: List[str]) -> np.ndarray:
        """
        Encode image inputs using base encoder + ONNX transformation.
        
        Args:
            image_paths: List of image file paths
            
        Returns:
            Array of transformed image embeddings
        """
        # Get base embeddings
        base_embeddings = self.base_encoder.encode_image(image_paths)
        
        # Apply ONNX transformation
        return self._apply_adapter(base_embeddings)
    

    def encode_dataset(
        self, texts: List[str], image_paths: List[str], 
        dataset_name: str, data_type: str, 
        use_cache: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        Encode a complete dataset with fine-tuning transformations.
        
        Args:
            texts: List of text strings
            image_paths: List of image paths
            dataset_name: Name of the dataset
            data_type: Type of data ('catalog' or 'test')
            use_cache: Whether to use cached embeddings
            
        Returns:
            Tuple of (transformed_text_embeddings, transformed_image_embeddings)
        """
        # Try to load from cache first
        if use_cache:
            cached_data = self.load_cached_embeddings(dataset_name, data_type)
            if cached_data is not None:
                return cached_data['text_embeddings'], cached_data['image_embeddings']
        
        # Generate base embeddings
        base_text_embeddings, base_image_embeddings = self.base_encoder.encode_dataset(
            texts, image_paths, dataset_name, data_type, use_cache=False
        )
        
        # Apply fine-tuning transformations
        transformed_text = self._apply_adapter(base_text_embeddings)
        transformed_image = self._apply_adapter(base_image_embeddings)
        
        # Save to cache
        if use_cache:
            self.save_cached_embeddings(
                dataset_name, 
                data_type, 
                transformed_text, 
                transformed_image
                )
        
        return transformed_text, transformed_image
    
    def get_config_hash(self) -> str:
        """Include model path in hash for caching."""
        config_str = f"{self.name}_{self.base_encoder_name}_{self.model_path}"
        return hashlib.md5(config_str.encode()).hexdigest()[:8]
    
    def transform_text_embeddings(self, text_embeddings: np.ndarray) -> np.ndarray:
        """
        Transform precomputed text embeddings using ONNX model.
        
        Args:
            text_embeddings: Precomputed text embeddings (n_texts, embedding_dim)
            
        Returns:
            Array of transformed text embeddings
        """
        return self._apply_adapter(text_embeddings)
    
    def transform_image_embeddings(self, image_embeddings: np.ndarray) -> np.ndarray:
        """
        Transform precomputed image embeddings using ONNX model.
        
        Args:
            image_embeddings: Precomputed image embeddings (n_images, embedding_dim)
            
        Returns:
            Array of transformed image embeddings
        """
        return self._apply_adapter(image_embeddings)
    
    def transform_embeddings_dataset(self, text_embeddings: np.ndarray, image_embeddings: np.ndarray,
                                   dataset_name: str, data_type: str, 
                                   use_cache: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        Transform precomputed embeddings for a complete dataset with caching support.
        
        Args:
            text_embeddings: Precomputed text embeddings
            image_embeddings: Precomputed image embeddings
            dataset_name: Name of the dataset
            data_type: Type of data ('catalog' or 'test')
            use_cache: Whether to use cached embeddings
            
        Returns:
            Tuple of (transformed_text_embeddings, transformed_image_embeddings)
        """
        # Try to load from cache first
        if use_cache:
            cached_data = self.load_cached_embeddings(dataset_name, data_type)
            if cached_data is not None:
                return cached_data['text_embeddings'], cached_data['image_embeddings']
        
        # Apply ONNX transformations to precomputed embeddings
        transformed_text = self._apply_adapter(text_embeddings)
        transformed_image = self._apply_adapter(image_embeddings)
        
        # Save to cache
        if use_cache:
            self.save_cached_embeddings(dataset_name, data_type, 
                                       transformed_text, transformed_image)
        
        return transformed_text, transformed_image
